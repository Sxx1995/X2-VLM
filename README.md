# X<sup>2</sup>-VLM: All-In-One Pre-trained Model For Vision-Language Tasks

<div align="center">
  <img width="70%" src="x2vlm_github.png">
</div>

X<sup>2</sup>-VLM with a modular architecture performs the best on base and large scale for both image-text and video-text tasks, making a good trade-off between performance and model scale. We also show that the modular design of X<sup>2</sup>-VLM results in high transferability for X<sup>2</sup>-VLM to be utilized in any language or domain. For example, by simply replacing the text encoder with XLM-R, X-VLM outperforms state-of-the-art multilingual multi-modal pre-trained models without any multilingual pre-training. 


- Jan 2023: We will release codes and pre-trained models in **Jan 2023**. 
- Nov 2022: Release preprint in arxiv. 


X<sup>2</sup>-VLM (large, 593M params): 
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/x-2-vlm-all-in-one-pre-trained-model-for/cross-modal-retrieval-on-flickr30k)](https://paperswithcode.com/sota/cross-modal-retrieval-on-flickr30k?p=x-2-vlm-all-in-one-pre-trained-model-for)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/x-2-vlm-all-in-one-pre-trained-model-for/cross-modal-retrieval-on-coco-2014)](https://paperswithcode.com/sota/cross-modal-retrieval-on-coco-2014?p=x-2-vlm-all-in-one-pre-trained-model-for)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/x-2-vlm-all-in-one-pre-trained-model-for/visual-grounding-on-refcoco-testa)](https://paperswithcode.com/sota/visual-grounding-on-refcoco-testa?p=x-2-vlm-all-in-one-pre-trained-model-for)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/x-2-vlm-all-in-one-pre-trained-model-for/visual-reasoning-on-nlvr2-test)](https://paperswithcode.com/sota/visual-reasoning-on-nlvr2-test?p=x-2-vlm-all-in-one-pre-trained-model-for)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/x-2-vlm-all-in-one-pre-trained-model-for/visual-question-answering-on-vqa-v2-test-std)](https://paperswithcode.com/sota/visual-question-answering-on-vqa-v2-test-std?p=x-2-vlm-all-in-one-pre-trained-model-for)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/x-2-vlm-all-in-one-pre-trained-model-for/visual-question-answering-on-msvd-qa-1)](https://paperswithcode.com/sota/visual-question-answering-on-msvd-qa-1?p=x-2-vlm-all-in-one-pre-trained-model-for)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/x-2-vlm-all-in-one-pre-trained-model-for/visual-question-answering-on-msrvtt-qa-1)](https://paperswithcode.com/sota/visual-question-answering-on-msrvtt-qa-1?p=x-2-vlm-all-in-one-pre-trained-model-for)


## Hiring
We are looking for interns / FTEs at ByteDance AI-LAB (in Beijing / Shanghai)! If you are interested in working with us on vision language models, please visit [here](https://job.toutiao.com/s/r9Ldf48).
